{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c3793bd",
   "metadata": {},
   "source": [
    "# Hybrid Classicalâ€“Quantum Transfer Learning for Cardiomegaly Detection in Chest X-rays\n",
    "\n",
    "## Classical-classical model based on DenseNet 121\n",
    "\n",
    "## Version with GradCAM++ heatmaps output for the test set\n",
    "\n",
    "\n",
    "Ref: J. Imaging 2023, 9, 128. https://doi.org/10.3390/jimaging9070128"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4cc027",
   "metadata": {},
   "source": [
    "## load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e06a4f-16fe-4aa0-a82c-1d3638bf0a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import random\n",
    "import os\n",
    "import copy\n",
    "        \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import ConcatDataset, Dataset\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import TensorDataset\n",
    "from nngeometry.layercollection import LayerCollection\n",
    "from nngeometry.metrics import FIM\n",
    "from nngeometry.object import PMatKFAC, PMatDense\n",
    "%matplotlib inline\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed_everything(123)\n",
    "    \n",
    "from watermark import watermark\n",
    "%reload_ext watermark\n",
    "%watermark \n",
    "%watermark --iversions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71148806-e60e-40b0-8824-515edeee4f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "n_qubits = 4                     # Number of qubits.\n",
    "q_depth = 6                      # Depth of the quantum circuit (number of variational layers).\n",
    "max_layers = 15                  # Keep 15 even if not all are used.\n",
    "q_delta = 0.01                   # Initial spread of random quantum weights.\n",
    "\n",
    "step = 10e-4                     # Learning rate.\n",
    "weight_decay = 10e-4             # Weight_decay for learning rate.\n",
    "batch_size = 8                   # Number of samples for each training step.\n",
    "init_epochs = 2                  # Number of init epochs.\n",
    "train_epochs = 18                # Number of training epochs.\n",
    "step_size= 2                     # Learning rate changing epochs.\n",
    "gamma_lr_scheduler = 0.3         # Learning rate reduction applied every step_size epochs.  \n",
    "\n",
    "start_time = time.time()         # Start of the computation timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0ce17b-8ff1-4869-83cf-cc7cb9084a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm \n",
    "import time\n",
    "start = time.time()\n",
    "img_size = 256\n",
    "\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize((img_size, img_size)), \n",
    "        transforms.CenterCrop((224,224)),\n",
    "        #transforms.RandomRotation(30),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomAutocontrast(p=0.5),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        #transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "    ]),\n",
    "}\n",
    "\n",
    "\n",
    "data_dir = 'chexpert-corrected/'\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
    "                     data_transforms[x]) for x in ['train', 'val']}\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
    "class_names = image_datasets['train'].classes\n",
    "\n",
    "# Initialize dataloader\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], \n",
    "                  batch_size=batch_size, shuffle=True) for x in ['train', 'val']}\n",
    "\n",
    "# function to plot images\n",
    "def imshow(inp, title=None):\n",
    "    \"\"\"Display image from tensor.\"\"\"\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    # We apply the inverse of the initial normalization operation.\n",
    "    if transforms.Normalize in data_transforms['val'].transforms:\n",
    "        norm = tranform_start[trans_list.index(transforms.Normalize)]\n",
    "        mean = np.array(norm.mean)#np.array([0.485, 0.456, 0.406])\n",
    "        std = np.array(norm.std)#np.array([0.229, 0.224, 0.225])\n",
    "        img = std * input_tensor + mean  \n",
    "    inp = np.clip(inp, 0, 1)\n",
    "    plt.imshow(inp)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "\n",
    "own_elapsed = time.time() - start\n",
    "print(\"Time elapsed: \", own_elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a29bedc-e76f-4faf-91b0-a09897cab05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a batch of training data\n",
    "inputs, classes = next(iter(dataloaders['val']))\n",
    "\n",
    "# Make a grid from batch\n",
    "out = torchvision.utils.make_grid(inputs)\n",
    "\n",
    "imshow(out, title=[class_names[x] for x in classes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283409d7-0786-4cfb-9456-2c462da96cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#resize image\n",
    "import cv2\n",
    "import PIL\n",
    "from glob import glob\n",
    "\n",
    "def read_img(img_path,tranform_):\n",
    "    img = cv2.imread(img_path)\n",
    "    transform = transforms.Compose(tranform_)\n",
    "    img = transform(PIL.Image.fromarray(img))\n",
    "    return img\n",
    "\n",
    "print('Scans found:', len(image_datasets['train'].imgs)+ len(image_datasets['val'].imgs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a5c4c0-a251-453b-b167-bbff02887458",
   "metadata": {},
   "source": [
    "### GradCam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3ff957-9d4b-445a-b03c-02364fb07640",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_grad_cam import GradCAM, HiResCAM, ScoreCAM, GradCAMPlusPlus, AblationCAM, XGradCAM, EigenCAM, FullGrad\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
    "from torchvision.models import resnet50,resnext50_32x4d,resnet18,densenet161,wide_resnet50_2, densenet121, densenet161\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb2f12b-a4df-410e-bd3e-4f79233a015b",
   "metadata": {},
   "source": [
    "### pre-process dataset\n",
    "generate image from address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee5e2a1-4b42-4ff7-981a-657fdae10f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "#device = 'cpu' #qiskit don't support gpu on windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c5382c-9264-48ea-93ce-208598b43636",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm \n",
    "train_img = []\n",
    "train_ids = []\n",
    "train_y = []\n",
    "\n",
    "img_size=256\n",
    "tranform_ = [transforms.Resize((img_size, img_size)), \n",
    "             transforms.CenterCrop((224,224)),\n",
    "             #transforms.RandomRotation(30),\n",
    "             #transforms.RandomHorizontalFlip(p=0.5),\n",
    "             #transforms.RandomAutocontrast(p=0.5),\n",
    "             #transforms.ToTensor(),\n",
    "             #transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "            ]\n",
    "\n",
    "train_img = []\n",
    "train_ids = []\n",
    "train_y = []\n",
    "\n",
    "for img_path in tqdm(image_datasets[\"train\"].imgs):\n",
    "    train_img.append(read_img(img_path[0],tranform_))\n",
    "    train_ids.append(img_path[0].split(\"\\\\\")[2])\n",
    "    train_y.append(img_path[1])\n",
    "    \n",
    "#valid_img = []\n",
    "#valid_ids = []\n",
    "#valid_y = []\n",
    "#\n",
    "#for img_path in tqdm(image_datasets[\"val\"].imgs):\n",
    "#    valid_img.append(read_img(img_path[0],tranform_))\n",
    "#    valid_ids.append(img_path[0].split(\"\\\\\")[2])\n",
    "#    valid_y.append(img_path[1])\n",
    "    \n",
    "test_img = []\n",
    "test_ids = []\n",
    "test_y = []\n",
    "for img_path in tqdm(image_datasets[\"val\"].imgs):\n",
    "    test_img.append(read_img(img_path[0],tranform_))\n",
    "    test_ids.append(img_path[0].split(\"\\\\\")[2])\n",
    "    test_y.append(img_path[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd75ad8-da28-4a4f-bb5b-b1a3efa68dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#qamp_IMAGE_DIR = \"qamp\"\n",
    "\n",
    "import time\n",
    "start = time.time()\n",
    "\n",
    "# * better way\n",
    "\n",
    "tranform_ = [#transforms.Resize((img_size, img_size)), \n",
    "             #transforms.CenterCrop((224,224)),\n",
    "             #transforms.RandomRotation(30),\n",
    "             #transforms.RandomHorizontalFlip(p=0.5),\n",
    "            #transforms.RandomVerticalFlip(p=0.5),\n",
    "             #transforms.RandomAutocontrast(p=1.0),\n",
    "             transforms.ToTensor(),\n",
    "             transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "            ]\n",
    "transform = transforms.Compose(tranform_)\n",
    "\n",
    "train_imgg = []\n",
    "for img in tqdm(train_img):\n",
    "    img = transform(img)\n",
    "    train_imgg.append(img)\n",
    "##train_ds=torch.tensor(np.array(train_img), device=device).float()\n",
    "train_y = torch.tensor(train_y, device=device).float()\n",
    "train_ds = TensorDataset(torch.stack(train_imgg).to(device), train_y)\n",
    "train_dataloader = torch.utils.data.DataLoader(train_ds, batch_size=8, shuffle=True)\n",
    "train_size = len(train_dataloader)\n",
    "\n",
    "tranform_ = [#transforms.Resize((img_size, img_size)), \n",
    "             #transforms.CenterCrop((224,224)),\n",
    "             #transforms.RandomRotation(30),\n",
    "             #transforms.RandomHorizontalFlip(p=0.5),\n",
    "             #transforms.RandomAutocontrast(p=1.0),\n",
    "             transforms.ToTensor(),\n",
    "             transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "            ]\n",
    "transform = transforms.Compose(tranform_)\n",
    "\n",
    "#valid_imgg=[]\n",
    "#for img in tqdm(valid_img):\n",
    "#    img = transform(img)\n",
    "#    valid_imgg.append(img)\n",
    "###valid_ds=torch.tensor(np.array(valid_img), device=device).float()\n",
    "#valid_y = torch.tensor(valid_df['Cardiomegaly'].values, device=device).float()\n",
    "#valid_ds = TensorDataset(torch.stack(valid_imgg).to(device), valid_y)\n",
    "#valid_dataloader = torch.utils.data.DataLoader(valid_ds, batch_size=2, shuffle=True)\n",
    "#valid_size = len(valid_dataloader)\n",
    "\n",
    "test_img_ = []\n",
    "for img in tqdm(test_img):\n",
    "    img = transform(img)\n",
    "    test_img_.append(img)\n",
    "#test_ds=torch.tensor(np.array(test_img_), device=device).float()\n",
    "test_y = torch.tensor(test_y, device=device).float()\n",
    "test_ds = TensorDataset(torch.stack(test_img_).to(device), test_y)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_ds, batch_size=1, shuffle=False)#, sampler= sampler)\n",
    "\n",
    "\n",
    "own_elapsed = time.time() - start\n",
    "print(\"Time elapsed: \", own_elapsed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9aa5a62-29a1-4f40-9d44-53d4ae89c3d1",
   "metadata": {},
   "source": [
    "### training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811fb3f7-7778-4e2b-a312-b191fae78722",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from torchsummary import summary as quick_sum # this is buggy, but light\n",
    "from torchinfo import summary # this is good, but take a lot of memory\n",
    "from sklearn.metrics import roc_auc_score, f1_score, precision_score, recall_score, accuracy_score, auc, roc_curve, accuracy_score,confusion_matrix,classification_report\n",
    "save_result = True\n",
    "if save_result == True:\n",
    "    # create folder and save the result\n",
    "    time_str = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "    result_OUT = f\"image_result/freezer_init_{time_str}\"\n",
    "    os.makedirs(result_OUT, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6d7d78-99d4-4f87-be7e-fabf619cb7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_variable_name(variable):\n",
    "    globals_dict = globals()\n",
    "\n",
    "    return [var_name for var_name in globals_dict if globals_dict[var_name] is variable]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed0ab31-43ce-411c-8538-444d7c3d0623",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conf_matrix(y, y_pred):\n",
    "    fig, ax =plt.subplots(figsize=(3.5,3.5))\n",
    "    labels=['No','Yes']\n",
    "    ax=sns.heatmap(confusion_matrix(y, y_pred), annot=True, cmap=\"Blues\", fmt='g', cbar=False, annot_kws={\"size\":25})\n",
    "    plt.title('Cardiomegaly?', fontsize=20)\n",
    "    ax.xaxis.set_ticklabels(labels, fontsize=17) \n",
    "    ax.yaxis.set_ticklabels(labels, fontsize=17)\n",
    "    ax.set_ylabel('Test')\n",
    "    ax.set_xlabel('Predicted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5db605-4f84-4d08-ae1f-68147f4f227f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_camera(models, target_layers, picture_select, transform_start=None, transform_end=None,name=None):\n",
    "    \"\"\" Train function\n",
    "    Args:\n",
    "        models: pytroch model\n",
    "        target_layers :  target layer of the model\n",
    "        picture_select (str): select picture from all the image\n",
    "        transform_start : input picture display\n",
    "        transform_end : output picture display\n",
    "    Returns:\n",
    "        pytorch model\n",
    "    Raises:\n",
    "        None\n",
    "    \"\"\"\n",
    "    #target_layers = target_layers\n",
    "    # need modify yourself\n",
    "    if transform_start == None:\n",
    "        tranform_start = [transforms.Resize((img_size, img_size)), \n",
    "                        transforms.CenterCrop((224,224)),\n",
    "                        #transforms.RandomRotation(30),\n",
    "                        #transforms.RandomHorizontalFlip(p=0.5),\n",
    "                        #transforms.RandomAutocontrast(p=0.5),\n",
    "                        transforms.ToTensor(),\n",
    "                        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "                        ]\n",
    "        #return tranform_start\n",
    "    input_tensor = torch.stack([read_img(eval(f'image_datasets[\"train\"].imgs{picture_select}[0]'), tranform_start)])#torch.stack(train_img)[:1,:,:,:]# Create an input tensor image for your model..\n",
    "    print(input_tensor.numpy()[0].shape)\n",
    "    y= models(input_tensor.to(device))\n",
    "    # Note: input_tensor can be a batch tensor with several images!\n",
    "    \n",
    "    # Construct the CAM object once, and then re-use it on many images:\n",
    "    cam = GradCAMPlusPlus(model=models, target_layers=target_layers)#, use_cuda=False)\n",
    "    \n",
    "    # You can also pass aug_smooth=True and eigen_smooth=True, to apply smoothing.\n",
    "    grayscale_cam = cam(input_tensor=input_tensor.to(device))[0, :]#, targets=targets)[0, :]\n",
    "    \n",
    "    # In this example grayscale_cam has only one image in the batch:\n",
    "    if transform_end == None:\n",
    "        trans_list = list(map(type, tranform_start))\n",
    "        input_tensor = input_tensor.numpy()[0]\n",
    "        input_tensor = np.transpose(input_tensor, (1,2,0))\n",
    "        if transforms.Normalize in trans_list:\n",
    "            #img = np.transpose(input_tensor, (1,2,0))\n",
    "            # We apply the inverse of the initial normalization operation.\n",
    "            norm = tranform_start[trans_list.index(transforms.Normalize)]\n",
    "            mean = np.array(norm.mean)#np.array([0.485, 0.456, 0.406])\n",
    "            std = np.array(norm.std)#np.array([0.229, 0.224, 0.225])\n",
    "            img = std * input_tensor + mean  \n",
    "            input_tensor = np.clip(img, 0, 1)\n",
    "    #img= read_img(eval(f'list(all_image_paths.values()){picture_select}'), transform_end)\n",
    "    #img =np.float32(img) / 255\n",
    "    visualization = show_cam_on_image(input_tensor, grayscale_cam, use_rgb=True)\n",
    "    \n",
    "    \n",
    "    #_, y_prob = torch.max(y, 1)\n",
    "    y_probb = nn.Sigmoid()(y)[0]\n",
    "    df = image_datasets[\"train\"].imgs\n",
    "    plt.title(f'Cardiomegaly?\\n true:{eval(f\"df{picture_select}[1]\")}\\n pred[no/yes]:{np.round(y_probb.cpu().detach().numpy(),3)}', fontsize=20)\n",
    "    #Image.fromarray(visualization, 'RGB')\n",
    "    plt.imshow(visualization)\n",
    "    if (save_result == True and name != None): plt.savefig(f\"{result_OUT}/{name}_visual_img.png\")\n",
    "    return visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d976ef-1934-48f0-9974-1ef138e23d20",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer,scheduler, num_epochs, loss_save=None, scheduler_set = [\"outside_train\"], sig_out = True,transform = True, save = True):\n",
    "    \"\"\" Train function\n",
    "    Args:\n",
    "        model : pytroch model\n",
    "        criterion :  Criterion\n",
    "        optimizer : optimize\n",
    "        scheduler : scheduler\n",
    "        num_epochs (int): Number of epochs\n",
    "        loss_save (bool): select model save condition, \n",
    "            it can be 'None' for best_loss_train, 'True' for best_acc or False for best_loss (default=None)\n",
    "        scheduler_set (list): The set the scheduler place, \n",
    "            it can include ('inside_train', 'outside_train', 'inside_valid', 'outside_valid', outside_loss_valid)\n",
    "        sig_out (bool): training sigmoid output\n",
    "        transform (bool): transform each epoch\n",
    "    Returns:\n",
    "        pytorch model\n",
    "    Raises:\n",
    "        None\n",
    "    \"\"\"\n",
    "    #qamp_IMAGE_DIR = \"qamp\"\n",
    "    start = time.time()\n",
    "    \n",
    "    #layer_collection = LayerCollection.from_model(model)\n",
    "    #d = layer_collection.numel()\n",
    "    #print('d= ', d)\n",
    "    name = get_variable_name(model)[0]\n",
    "    print('name:',name)\n",
    "    \n",
    "    #train_ds=torch.tensor(np.array(train_img), device=device).float()\n",
    "    \n",
    "    \n",
    "    #own_elapsed = time.time() - start\n",
    "    #print(\"loader Time elapsed: \", own_elapsed)\n",
    "    global train_ds\n",
    "    global train_dataloader\n",
    "    #global valid_ds\n",
    "    #global valid_dataloader\n",
    "    \n",
    "    train_loss = []\n",
    "    train_loss_list = []\n",
    "    training_loss = []\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    best_loss = 10000.0   # Large arbitrary number\n",
    "    best_acc_train = 0.0\n",
    "    best_loss_train = 10000.0  # Large arbitrary number\n",
    "    opt_rate=[]\n",
    "    for epoch in range(num_epochs):\n",
    "        if transform:\n",
    "            trans_start = time.time()\n",
    "            tranform_ = [#transforms.Resize((img_size, img_size)), \n",
    "                 #transforms.CenterCrop((224,224)),\n",
    "                 #transforms.RandomRotation(30),\n",
    "                 #transforms.RandomHorizontalFlip(p=.5),\n",
    "                 #transforms.RandomVerticalFlip(p=.5),\n",
    "                 transforms.RandAugment(),\n",
    "                 #transforms.RandomAdjustSharpness(sharpness_factor=2),\n",
    "                 #transforms.RandomSolarize(threshold=192.0),\n",
    "                 #transforms.RandomSolarize(threshold=192.0),\n",
    "                 #transforms.RandomInvert(),\n",
    "                 transforms.RandomAutocontrast(p=.5),\n",
    "                 transforms.ToTensor(),\n",
    "                 transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "                ]\n",
    "            transform = transforms.Compose(tranform_)\n",
    "            \n",
    "            train_imgg = []\n",
    "            for img in train_img:\n",
    "                img = transform(img)\n",
    "                train_imgg.append(img)\n",
    "            \n",
    "            train_ds = TensorDataset(torch.stack(train_imgg).to(device), train_y)\n",
    "            train_dataloader = torch.utils.data.DataLoader(train_ds, batch_size=8, shuffle=True)\n",
    "            \n",
    "            #valid_imgg=[]\n",
    "            #for img in valid_img:\n",
    "            #    img = transform(img)\n",
    "            #    valid_imgg.append(img)\n",
    "            #valid_ds = TensorDataset(torch.stack(valid_imgg).to(device), valid_y)\n",
    "            #valid_dataloader = torch.utils.data.DataLoader(valid_ds, batch_size=2, shuffle=False)\n",
    "            \n",
    "            print(\"Transform Time elapsed: \", time.time() - trans_start)\n",
    "        \n",
    "        train_size = len(train_dataloader)\n",
    "        #valid_size = len(valid_dataloader)\n",
    "        \n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "        print(epoch+1,'Training started:')\n",
    "        #for dataset_size_multiplier in range(5):\n",
    "        for index, data in enumerate(tqdm(train_dataloader)):\n",
    "            inputs, labels = data[0].to(device).type(torch.float), data[1].to(device).type(torch.long)\n",
    "            #running_loss = 0.0\n",
    "            # Set model to training mode\n",
    "            model.train() \n",
    "            \n",
    "            # Each epoch has a training and validation phase\n",
    "            batch_size_ = len(inputs)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Iterate over data.\n",
    "            #n_batches = dataset_sizes[phase] // batch_size\n",
    "            # Track/compute gradient and make an optimization step only when training\n",
    "            with torch.set_grad_enabled(True):\n",
    "                outputs = model(inputs)\n",
    "                #nn.sigmoid(outputs)\n",
    "                #print(outputs)\n",
    "                if len(outputs.shape) == 1:\n",
    "                    outputs = torch.stack([outputs])\n",
    "                if sig_out == True:\n",
    "                    outputs = nn.Sigmoid()(outputs)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                loss = criterion(outputs, labels)#torch.stack(label_list))\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                if \"inside_train\" in scheduler_set:\n",
    "                    scheduler.step()\n",
    "                \n",
    "            \n",
    "            # Print iteration results\n",
    "            running_loss += loss.item() #*inputs.size(0)#* batch_size_\n",
    "            train_loss.append(loss.item() )#*inputs.size(0))#*batch_size_)\n",
    "            batch_corrects = torch.sum(preds == labels.data).item()\n",
    "            running_corrects += batch_corrects\n",
    "            if (index+1)%train_size == 0:\n",
    "                #print(outputs)\n",
    "                #print(preds)\n",
    "                print('Train Epoch: {}/{} train loss {:.4f} Acc batch: {:.4f} learning_rate: {:.4f}'.format(epoch + 1, num_epochs, running_loss/train_size, running_corrects/train_ds.tensors[0].size(0),optimizer.state_dict()['param_groups'][0]['lr']))\n",
    "                training_loss.append(running_loss/train_size)\n",
    "        if running_corrects/len(train_ds) > best_acc_train:\n",
    "            best_acc_train = running_loss\n",
    "        if running_loss < best_loss_train:\n",
    "            best_loss_train = running_corrects/len(train_ds)#round(new_train_df.shape[0]*split_size)\n",
    "        if \"outside_train\" in scheduler_set:\n",
    "            scheduler.step()\n",
    "                \n",
    "        train_loss_list.append(np.mean(train_loss))\n",
    "        #opt_rate.append(optimizer.state_dict()['param_groups'][0]['lr'])\n",
    "    if save_result: np.savetxt(f\"{result_OUT}/{name}_training_loss.csv\", training_loss)#save\n",
    "    if save_result: np.savetxt(f\"{result_OUT}/{name}_train_loss.csv\", train_loss)#save\n",
    "    if save_result: np.savetxt(f\"{result_OUT}/{name}_train_loss_list.csv\", train_loss_list)#save\n",
    "            \n",
    "    own_elapsed = time.time() - start\n",
    "    print(\"Time elapsed: \", own_elapsed)\n",
    "    \n",
    "    # Print final results \n",
    "    #if loss_save != None:\n",
    "    #    model.load_state_dict(best_model_wts)\n",
    "    #print('Best test loss: {:.4f} | Best test accuracy: {:.4f}'.format(best_loss, best_acc))\n",
    "    if len(train_loss_list) > 1:\n",
    "        plt.rcParams[\"figure.figsize\"] = (5.5, 4)\n",
    "        plt.title(\"Training loss against Epoch\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Training loss\")\n",
    "        plt.plot(range(0,len(train_loss_list)), train_loss_list, color=\"blue\")\n",
    "        if save_result == True: plt.savefig(f\"{result_OUT}/{name}_train_loss.png\")#save pic\n",
    "        plt.show()\n",
    "    if save:\n",
    "        running_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            \n",
    "            y_pred = []\n",
    "            for index, data in enumerate(tqdm(test_dataloader)):\n",
    "                model.eval()\n",
    "                batch_inputs, batch_labels = data[0].to(device).type(torch.float), data[1].to(device).type(torch.long)\n",
    "                with torch.set_grad_enabled(False):\n",
    "                    outputs = model(batch_inputs)\n",
    "                    y_pred.append(outputs[0])\n",
    "                    #assert isinstance(outputs.item(), float)\n",
    "                    #print(outputs, batch_labels)\n",
    "                    #if len(outputs.shape) == 1:\n",
    "                    #    outputs = torch.stack([outputs])\n",
    "                    if sig_out == True:\n",
    "                        outputs = nn.Sigmoid()(outputs)\n",
    "                    #_, preds = torch.max(outputs, 1)\n",
    "                    loss    = criterion(outputs, batch_labels)\n",
    "                #print(f\"Step {index} loss: {loss}\")\n",
    "                running_loss += loss.item()\n",
    "            _, y_pred_prob = torch.max(torch.stack(y_pred), 1)\n",
    "            y_pred_prob = y_pred_prob.cpu()\n",
    "            if save_result == True: np.savetxt(f\"{result_OUT}/{name}_test_loss.csv\", y_pred_prob)#save\n",
    "            #print(y_pred, y_pred_prob)\n",
    "            print(classification_report(test_y.cpu().long(), torch.tensor(y_pred_prob.detach().numpy()),digits=4))\n",
    "            conf_matrix(test_y.cpu().long(), y_pred_prob)\n",
    "            if save_result == True: plt.savefig(f\"{result_OUT}/{name}_conf_matrix.png\")#save pic\n",
    "            plt.figure(figsize = (5.5, 4))\n",
    "            fpr, tpr, _ =roc_curve(test_y.cpu().long(), torch.tensor(y_pred_prob.detach().numpy()))\n",
    "            roc_auc = auc(fpr, tpr)\n",
    "            plt.plot(fpr, tpr, 'b', label = 'AUC = %0.4f' % roc_auc)\n",
    "            plt.plot([0, 1], [0, 1],'r--')\n",
    "            plt.title('ROC curve',fontsize=25)\n",
    "            plt.ylabel('True Positive Rate',fontsize=18)\n",
    "            plt.xlabel('False Positive Rate',fontsize=18)\n",
    "            plt.legend(loc = 'lower right', fontsize=24, fancybox=True, shadow=True, frameon=True, handlelength=0)\n",
    "            if save_result == True: plt.savefig(f\"{result_OUT}/{name}_roc.png\") #save pic\n",
    "            plt.show()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b687a6",
   "metadata": {},
   "source": [
    "## init_freezer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0f01cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "def n_count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if not(p.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a876d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_func(cnn_model,init_step, train_step,save_model=save_result):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    if init_step > 0:\n",
    "        optimizer = torch.optim.Adam(cnn_model.parameters(), lr=step, weight_decay = weight_decay)\n",
    "        #optimizer = torch.optim.SGD(filter(lambda p: p.requires_grad, cnn_model.parameters()), lr=10e-4, momentum=0.9)\n",
    "        exp_lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma_lr_scheduler)\n",
    "        cnn_model = train_model(cnn_model.to(device), \n",
    "                                criterion, optimizer,exp_lr_scheduler, \n",
    "                                init_step,None,[\"outside_train\"], False, True,False)\n",
    "    \n",
    "    for param in cnn_model.parameters():\n",
    "        param.requires_grad = True\n",
    "    \n",
    "    optimizer = torch.optim.Adam(cnn_model.parameters(), lr=(step*0.3)*(np.floor(init_step/2)) if init_step > 0  else step, weight_decay = weight_decay)\n",
    "    #optimizer = torch.optim.SGD(filter(lambda p: p.requires_grad, cnn_model.parameters()), lr=3e-4, momentum=0.9)\n",
    "    exp_lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma_lr_scheduler)\n",
    "    cnn_model = train_model(cnn_model.to(device), \n",
    "                            criterion, optimizer,exp_lr_scheduler, \n",
    "                            train_step,None,[\"outside_train\"], False, True)#[\"outside_train\"]\n",
    "    \n",
    "    cnn_model_visual = grad_camera(cnn_model, [cnn_model.features[-1]],[0] ,transform_start=None, transform_end=None,name=get_variable_name(cnn_model)[0])\n",
    "    if save_model:\n",
    "        torch.save(cnn_model.state_dict(),f\"{result_OUT}/{get_variable_name(cnn_model)[0]}.pt\")\n",
    "    return cnn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9acba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob_loop(sample,models, target_layers, picture_select, transform_start=None, transform_end=None,name=None):\n",
    "    # need modify yourself\n",
    "    if transform_start == None:\n",
    "        tranform_start = [transforms.Resize((img_size, img_size)), \n",
    "                        transforms.CenterCrop((224,224)),\n",
    "                        #transforms.RandomRotation(30),\n",
    "                        #transforms.RandomHorizontalFlip(p=0.5),\n",
    "                        #transforms.RandomAutocontrast(p=0.5),\n",
    "                        transforms.ToTensor(),\n",
    "                        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "                        ]\n",
    "    input_tensor = torch.stack([read_img(eval(f'image_datasets[sample].imgs{picture_select}[0]'), tranform_start)])#torch.stack(val_img)[:1,:,:,:]# Create an input tensor image for your model..\n",
    "    y= models(input_tensor.to(device))\n",
    "    \n",
    "    y_probb = nn.Sigmoid()(y)[0]\n",
    "    return y_probb\n",
    "\n",
    "\n",
    "def make_df_y_probb(sample,model) :  \n",
    "    df_y_probb = pd.DataFrame(columns=['image', 'p_yes', 'p_no'])\n",
    "    df = image_datasets[sample].imgs\n",
    "    for i in tqdm(range(len(df))):\n",
    "        image_name = df[i][0][-10:]\n",
    "        y_probb = prob_loop(sample, model, [model.features[-1]] \\\n",
    "            ,[i] ,transform_start=None, transform_end=None)\n",
    "        y_probb = y_probb.cpu()\n",
    "        p_yes = y_probb.detach().numpy()[0]\n",
    "        p_no = y_probb.detach().numpy()[1]\n",
    "        row = {'image' : image_name, 'p_yes': p_yes, 'p_no': p_no}\n",
    "        new_df = pd.DataFrame([row])\n",
    "        df_y_probb =  pd.concat([df_y_probb, new_df], axis = 0, ignore_index=True)\n",
    "    return df_y_probb\n",
    "\n",
    "def plot_ROC(name,y_true, y_score):\n",
    "\n",
    "    # ROC Curve and AUROC\n",
    "    plt.figure(0).clf()\n",
    "    if np.sum(y_true) != 0.:        \n",
    "        fpr, tpr, _ = roc_curve(y_true, y_score)\n",
    "        auc = round(roc_auc_score(y_true, y_score), 4)\n",
    "        plt.plot(fpr,tpr,label=name + \", AUC =\" + str(auc))\n",
    "    if save_result == True: plt.savefig(f\"{result_OUT}/{name}_roc.png\") #save pic\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def make_csv(model, sample=\"val\",result=True):\n",
    "    df = make_df_y_probb(sample,model)\n",
    "    if sample == \"val\":\n",
    "        df_list = 361*[1] + 369*[0]\n",
    "    elif sample == \"train\":\n",
    "        df_list = 850*[1] + 856*[0]\n",
    "    df = df.assign(label = df_list)\n",
    "\n",
    "    pred_list = np.array(df.p_yes) / (np.array(df.p_yes) + np.array(df.p_no))\n",
    "    df = df.assign(cmg_pred = pred_list)\n",
    "    #elif sample == \"train\":\n",
    "    #    pred_val_list =( np.array(df.p_yes) / (np.array(df.p_yes) + np.array(df.p_no)) + 0.5).astype('int32')\n",
    "    #    df = df.assign(pred = pred_val_list)\n",
    "    \n",
    "    pred_list =( np.array(df.p_yes) / (np.array(df.p_yes) + np.array(df.p_no)) + 0.5).astype('int32')\n",
    "    df = df.assign(pred = pred_list)\n",
    "    \n",
    "    df = df.rename(columns={\"cmg_pred\": \"prob\"})\n",
    "    if sample == \"val\":\n",
    "        df.sort_values(by = 'image')[0:16]\n",
    "    elif sample == \"train\":\n",
    "        df.sort_values(by = 'image')\n",
    "    if save_result: df.to_csv(f'{result_OUT}/y_probbs_{get_variable_name(model)[0]}_{sample}.csv', index = False)\n",
    "    \n",
    "    if result:\n",
    "        name = get_variable_name(model)[0]\n",
    "        df_val = pd.read_csv(f'{result_OUT}/y_probbs_{name}_{sample}.csv') \n",
    "        prediction = np.array(df_val.prob) \n",
    "        name_ROC = f\"Cardiomegaly - {name}_{sample}\"\n",
    "        plot_ROC(name_ROC, np.array(df_val.label), prediction)\n",
    "        print(classification_report(np.array(df_val.label),  (prediction+ 0.5).astype('int32')))#prediction))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36fc0206",
   "metadata": {},
   "source": [
    "### densenet121\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e795e3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "densenet121_model = torchvision.models.densenet121(weights= 'DEFAULT')\n",
    "for param in densenet121_model.parameters():\n",
    "    param.requires_grad = False\n",
    "densenet121_model.classifier = nn.Sequential(nn.Linear(densenet121_model.classifier.in_features, 512), torch.nn.ReLU(), nn.Linear(512, 2))\n",
    "densenet121_model = train_func(densenet121_model, init_epochs, train_epochs)\n",
    "\n",
    "for i in ['train','val']:\n",
    "    exec(f\"densenet121_model_{i}=make_csv(densenet121_model,'{i}')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54d517c",
   "metadata": {},
   "source": [
    " ### Obtain GradCAM++ heatmaps for all test set chest X-rays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdac2245",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = image_datasets[\"val\"].imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4e056e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_cam_loop(sample,models, target_layers, picture_select, transform_start=None, transform_end=None,name=None):\n",
    "    \"\"\" function\n",
    "    Args:\n",
    "        models: pytroch model\n",
    "        target_layers :  target layer of the model\n",
    "        picture_select (str): select picture from all the image\n",
    "        transform_start : input picture display\n",
    "        transform_end : output picture display\n",
    "    Returns:\n",
    "        pytorch model\n",
    "    Raises:\n",
    "        None\n",
    "    \"\"\"\n",
    "    #target_layers = target_layers\n",
    "    # need modify yourself\n",
    "    if transform_start == None:\n",
    "        tranform_start = [transforms.Resize((img_size, img_size)), \n",
    "                        transforms.CenterCrop((224,224)),\n",
    "                        #transforms.RandomRotation(30),\n",
    "                        #transforms.RandomHorizontalFlip(p=0.5),\n",
    "                        #transforms.RandomAutocontrast(p=0.5),\n",
    "                        transforms.ToTensor(),\n",
    "                        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "                        ]\n",
    "        #return tranform_start\n",
    "    input_tensor = torch.stack([read_img(eval(f'image_datasets[sample].imgs{picture_select}[0]'), tranform_start)])#torch.stack(val_img)[:1,:,:,:]# Create an input tensor image for your model..\n",
    "    #print(input_tensor.numpy()[0].shape)\n",
    "    y= models(input_tensor)\n",
    "    # Note: input_tensor can be a batch tensor with several images!\n",
    "    \n",
    "    # Construct the CAM object once, and then re-use it on many images:\n",
    "    cam = GradCAMPlusPlus(model=models, target_layers=target_layers)#, use_cuda=False)\n",
    "    \n",
    "    # You can also pass aug_smooth=True and eigen_smooth=True, to apply smoothing.\n",
    "    grayscale_cam = cam(input_tensor=input_tensor)[0, :]#, targets=targets)[0, :]\n",
    "    \n",
    "    # In this example grayscale_cam has only one image in the batch:\n",
    "    if transform_end == None:\n",
    "        trans_list = list(map(type, tranform_start))\n",
    "        input_tensor = input_tensor.numpy()[0]\n",
    "        input_tensor = np.transpose(input_tensor, (1,2,0))\n",
    "        if transforms.Normalize in trans_list:\n",
    "            #img = np.transpose(input_tensor, (1,2,0))\n",
    "            # We apply the inverse of the initial normalization operation.\n",
    "            norm = tranform_start[trans_list.index(transforms.Normalize)]\n",
    "            mean = np.array(norm.mean)#np.array([0.485, 0.456, 0.406])\n",
    "            std = np.array(norm.std)#np.array([0.229, 0.224, 0.225])\n",
    "            img = std * input_tensor + mean  \n",
    "            input_tensor = np.clip(img, 0, 1)\n",
    "    \n",
    "    visualization = show_cam_on_image(input_tensor, grayscale_cam, use_rgb=True)\n",
    "    \n",
    "    return visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f32c295",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = \"val\"\n",
    "df = image_datasets[sample].imgs\n",
    "\n",
    "# note: 0 is for cardiomegaly by convention in this notebook, \n",
    "# but not in the manuscript where it was corrected to be 1\n",
    "\n",
    "for i in range(len(df)):\n",
    "    image_name = df[i][0][-10:]\n",
    "    if df[i][1] == 0: \n",
    "        subfolder = \"cardiomegaly\"\n",
    "    else:\n",
    "        subfolder = \"control\"\n",
    "    #print(image_name)\n",
    "    densenet121_model_visual = grad_cam_loop(sample, densenet121_model, [densenet121_model.features.norm5] ,[i] ,transform_start=None, transform_end=None)\n",
    "    plt.imshow(densenet121_model_visual)\n",
    "    gcpath = \"gcnn/\" + sample + \"/\" + subfolder + \"/gc_\" + image_name\n",
    "    if i%25 == 0:\n",
    "        print(i)\n",
    "    plt.savefig(gcpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32326cd7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "781px",
    "left": "50px",
    "top": "111.141px",
    "width": "196.797px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "vscode": {
   "interpreter": {
    "hash": "4917f613eb6058c6158af0b40aab161702a6ca1d0f495cc551c9dbbb6c63dbba"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
